import asyncio
import base64
import requests
import json
import os
from typing import List, Optional
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.utils import EmbeddingFunc

class OptimizedOllamaLocal:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.text_model = "gpt-oss:20b"  # Your text model
        self.vision_model = "llava:34b"  # Your vision model
        self.embedding_model = "nomic-embed-text"  # Recommended embedding model
        
    def _check_model_available(self, model: str) -> bool:
        """Check if a specific model is available"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            response.raise_for_status()
            available_models = [m["name"] for m in response.json().get("models", [])]
            return any(model in available_model for available_model in available_models)
        except:
            return False
    
    def text_completion(self, prompt: str, system_prompt: Optional[str] = None, 
                       history_messages: List = [], **kwargs):
        """Text completion using gpt-oss:20b"""
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.extend(history_messages)
        messages.append({"role": "user", "content": prompt})
        
        payload = {
            "model": self.text_model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": kwargs.get("temperature", 0.1),
                "top_p": kwargs.get("top_p", 0.9),
                "top_k": kwargs.get("top_k", 40),
                "num_predict": kwargs.get("max_tokens", 4096),  # Increased for larger context
            }
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/api/chat",
                json=payload,
                timeout=kwargs.get("timeout", 300)  # Longer timeout for larger model
            )
            response.raise_for_status()
            return response.json()["message"]["content"]
        except requests.exceptions.Timeout:
            return "Error: Request timeout - model may be processing large context"
        except Exception as e:
            return f"Error in text completion: {str(e)}"
    
    def vision_completion(self, prompt: str, image_path: Optional[str] = None, 
                         image_data: Optional[bytes] = None, system_prompt: Optional[str] = None, **kwargs):
        """Vision completion using llava:34b"""
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        content = [{"type": "text", "text": prompt}]
        
        # Handle image data
        if image_data:
            if isinstance(image_data, str):
                image_base64 = image_data
            else:
                image_base64 = base64.b64encode(image_data).decode('utf-8')
        elif image_path and os.path.exists(image_path):
            with open(image_path, "rb") as img_file:
                image_base64 = base64.b64encode(img_file.read()).decode('utf-8')
        else:
            # No image provided, fall back to text model
            return self.text_completion(prompt, system_prompt, **kwargs)
        
        content.append({
            "type": "image", 
            "source": {
                "type": "base64",
                "media_type": "image/jpeg",
                "data": image_base64
            }
        })
        
        messages.append({"role": "user", "content": content})
        
        payload = {
            "model": self.vision_model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": kwargs.get("temperature", 0.1),
                "num_predict": kwargs.get("max_tokens", 2048),
            }
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/api/chat",
                json=payload,
                timeout=kwargs.get("timeout", 300)
            )
            response.raise_for_status()
            return response.json()["message"]["content"]
        except Exception as e:
            return f"Error in vision completion: {str(e)}"
    
    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Get embeddings using nomic-embed-text"""
        embeddings = []
        for text in texts:
            payload = {
                "model": self.embedding_model,
                "prompt": text
            }
            try:
                response = requests.post(
                    f"{self.base_url}/api/embeddings",
                    json=payload,
                    timeout=60
                )
                response.raise_for_status()
                embedding = response.json().get("embedding", [])
                embeddings.append(embedding)
            except Exception as e:
                print(f"Embedding error: {e}")
                # Create simple fallback embedding
                embeddings.append([0.0] * 768)
        
        return embeddings

async def main():
    """Main function optimized for your specific models"""
    
    # Initialize optimized Ollama client
    ollama = OptimizedOllamaLocal()
    
    # Verify models are available
    print("Checking available models...")
    if not ollama._check_model_available(ollama.text_model):
        print(f"‚ùå Text model '{ollama.text_model}' not found. Please pull it first.")
        return
    
    if not ollama._check_model_available(ollama.vision_model):
        print(f"‚ùå Vision model '{ollama.vision_model}' not found. Please pull it first.")
        return
    
    if not ollama._check_model_available(ollama.embedding_model):
        print(f"‚ùå Embedding model '{ollama.embedding_model}' not found. Please pull it first.")
        return
    
    print("‚úÖ All models available!")
    
    # Configure RAGAnything for optimal performance with your models
    config = RAGAnythingConfig(
        working_dir="./rag_oss_llava",
        parser="docling",  # Better for complex document layouts
        parse_method="auto",
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
        chunk_size=1024,  # Optimized for gpt-oss context
        chunk_overlap=100,
        max_chunk_size=2048,
    )

    # Model functions using your specific models
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return ollama.text_completion(
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            **kwargs
        )

    def vision_model_func(prompt, system_prompt=None, image_data=None, **kwargs):
        return ollama.vision_completion(
            prompt,
            image_data=image_data,
            system_prompt=system_prompt,
            **kwargs
        )

    # Embedding function
    def ollama_embedding_func(texts):
        return ollama.get_embeddings(texts)

    embedding_func = EmbeddingFunc(
        embedding_dim=768,  # nomic-embed-text dimension
        max_token_size=8192,
        func=ollama_embedding_func
    )

    # Initialize RAG
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )

    # Process documents
    document_path = "path/to/your/document.pdf"  # Change this to your document
    
    if os.path.exists(document_path):
        print(f"üîÑ Processing {document_path}...")
        await rag.process_document_complete(
            file_path=document_path,
            output_dir="./oss_llava_output",
            parse_method="auto"
        )
        print("‚úÖ Document processing complete!")
    else:
        print(f"‚ùå Document not found: {document_path}")
        # Create a test with sample files if available
        sample_files = [f for f in os.listdir('.') if f.lower().endswith(('.pdf', '.docx', '.pptx', '.txt'))]
        if sample_files:
            print(f"Found sample files: {sample_files}")
            document_path = sample_files[0]
            await rag.process_document_complete(
                file_path=document_path,
                output_dir="./oss_llava_output",
                parse_method="auto"
            )

    # Interactive query session
    print("\nüéØ Ready for queries! (Type 'quit' to exit)")
    
    while True:
        try:
            query = input("\nü§î Enter your query: ")
            if query.lower() in ['quit', 'exit', 'q']:
                break
            
            if not query.strip():
                continue
            
            print("üîÑ Processing...")
            
            # Determine if this is likely an image-related query
            image_keywords = ['image', 'picture', 'photo', 'figure', 'chart', 'graph', 'diagram', 'screenshot']
            has_image_context = any(keyword in query.lower() for keyword in image_keywords)
            
            if has_image_context:
                print("üì∑ Using multimodal capabilities...")
                result = await rag.aquery(query, mode="hybrid")
            else:
                result = await rag.aquery(query, mode="hybrid")
            
            print(f"\nüí° Answer: {result}")
            
        except KeyboardInterrupt:
            print("\nüëã Exiting...")
            break
        except Exception as e:
            print(f"‚ùå Error during query: {e}")

if __name__ == "__main__":
    # Check if Ollama is running
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        print("‚úÖ Ollama is running! Starting RAGAnything with your models...")
        asyncio.run(main())
    except Exception as e:
        print(f"""
‚ö†Ô∏è  Ollama is not running or not accessible!

Error: {e}

Please ensure:
1. Ollama is installed and running
2. Your models are pulled:

# Pull your specific models
ollama pull gpt-oss:20b
ollama pull llava:34b
ollama pull nomic-embed-text

3. Then run this script again.
        """)
